{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* THIS IS THE MICROGRAD NOTBOOK WHICH STARTS WITH IMPLEMENTING THE VARIENTS OF TANH *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import Any\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let move to nn, we should start by building basic datastructures\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label='') -> None:\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # grad is initialized to 0 assuming in the beginning there is no impact of changing 'data' on the output 'L'\n",
    "        self._backward = lambda: None # backward ops at each node\n",
    "        self._prev = set(_children) # children is to keep the memory of previous nodes\n",
    "        self._op = _op # op is the operation done on the nodes\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # the other operand is wrapped in Value\n",
    "        out = Value(self.data + other.data, (self, other), '+') # (self, other) is a tuple and passed as children\n",
    "\n",
    "        # lets define the backward at the add node (secret: from 'out' it flows into 'self' and 'other')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other): # reverse add\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # the other operand is wrapped in Value\n",
    "        out = Value(self.data * other.data, (self, other), '*') # (self, other) is a tuple and passed as children\n",
    "        \n",
    "        # lets define the backward at the mul node (secret: out.grad a.k.a global gradient is multiplied with \n",
    "        # local gradients of 'self' and 'other', where local(self.grad) = other.data and local(other.grad) = self.data)\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other): # reverse multiply\n",
    "        return self * other\n",
    "    \n",
    "    # remember that substraction and negate use the 'add' and 'mul' ops\n",
    "    def __neg__(self): # negation, -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __pow__(self, other): # other is the pow value\n",
    "        assert(isinstance(other, (int, float)))\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad # derivative rules - https://www.google.com/search?sca_esv=589070032&rlz=1C5CHFA_enGB1040GB1040&sxsrf=AM9HkKmzNBy5PI7mT-5g3DqaPLwo24rheQ:1702036879826&q=derivative+rules&tbm=isch&source=lnms&sa=X&sqi=2&ved=2ahUKEwiH9_a85f-CAxU1iv0HHTi3DU0Q0pQJegQIDRAB&biw=1470&bih=803&dpr=2#imgrc=wXvz5V_wFuJ6jM\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        # self / other\n",
    "        return self * other**-1 # other to the pow is implemented above\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward=_backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        # lets define a func for handling exp\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad # https://www.youtube.com/watch?v=RtGjBRIwONA&t=121s = e(x)\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        # lets now automate the autograd based on topo\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v) # all the childrens are first added to the list, later o is added\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0)\n",
    "a.exp()\n",
    "\n",
    "b = Value(4.0)\n",
    "a/b\n",
    "\n",
    "b - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement division following will be considered\n",
    "a / b\n",
    "a * (1/b)\n",
    "a * (b**-1)\n",
    "\n",
    "# so lets implement something powerfull like (b**-1) i.e. 'b' to the power '-1'\n",
    "x**k # 'x' to the power some 'k', 'k' will be int or float, we should be able to differentiate x**k, and 'k'=-1 is the division special case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to now visualize the operations graph (bit complicated)\n",
    "# lets use graphviz\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in the graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = Left to Right\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # create the root node\n",
    "        dot.node(name=uid, label=\"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape='record')\n",
    "        # create a fake op node\n",
    "        if n._op:\n",
    "            dot.node(name=uid + n._op, label=n._op)\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    # todo\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets develop a neuron\n",
    "\n",
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1 * w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2 * w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "#draw_dot(n)\n",
    "\n",
    "# we will now apply activation function - tanh, but tanh cannot be applied on +*, since tanh is hyperbolic function we will need exponensiation\n",
    "# https://en.wikipedia.org/wiki/Hyperbolic_functions\n",
    "# tanh x = sinh x\\cosh x = e^{x}-e^{-x} / e^{x}+e^{-x}= e^{2x}-1 / e^{2x}+1\n",
    "\n",
    "o = n.tanh(); o.label='o'\n",
    "\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check with autograd\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the implementation of exp, pow, div, sub. It is time to break tanh above and use 'e' to the power rule instead\n",
    "\n",
    "# lets develop a neuron\n",
    "\n",
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1 * w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2 * w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "#draw_dot(n)\n",
    "\n",
    "# we will now apply activation function - tanh, but tanh cannot be applied on +*, since tanh is hyperbolic function we will need exponensiation\n",
    "# https://en.wikipedia.org/wiki/Hyperbolic_functions\n",
    "# tanh x = sinh x\\cosh x = e^{x}-e^{-x} / e^{x}+e^{-x}= e^{2x}-1 / e^{2x}+1\n",
    "\n",
    "# ------------\n",
    "e = (2*n).exp()\n",
    "o = (e - 1) / (e + 1)\n",
    "# ------------\n",
    "\n",
    "o.label='o'\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now use pytorch to do the above\n",
    "import torch\n",
    "\n",
    "# torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# casting to double for dtype to be float64\n",
    "x1 = torch.Tensor([2.0]).double()               ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()               ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()              ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()               ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('-----')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to build two layer perceptron\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "# single neuron\n",
    "class Neuron:\n",
    "    # nin is number of inputs\n",
    "    # no. of 'w' weights are equal to no. of inputs like x1w1, x2w2, x3w3 and so on\n",
    "    def __init__(self, nin) -> None:\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    # forward pass\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        # zip(self.w, x) # zip iterates over a tuple (w, x) in the given arrs\n",
    "        #print(list(zip(self.w, x)))\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b) # neuron activation function\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    # parameters list for a neuron which is 'w' and 'b'\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "# lets define the layer now\n",
    "class Layer:\n",
    "    # nin is no. of inputs to a single neuron\n",
    "    # nout is no. of neuron in a layer\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    # iterate each neuron in the layer and pass input 'x' to get the output of each neuron, outs=list of outputs\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs # return single output else list\n",
    "    \n",
    "    # parameters list for a layer\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            ps = neuron.parameters()\n",
    "            params.extend(ps)\n",
    "        return params\n",
    "\n",
    "# lets define a MLP - multi layer perceptron\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts) -> None: # (self, 3, [4, 4, 1])\n",
    "        sz = [nin] + nouts # [3, 4, 4, 1]\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()] # simplified expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.6775668414781266)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x = [2.0, 3.0]\n",
    "#n = Neuron(2) # 2 because the inputs are 2\n",
    "#n = Layer(2, 3) # 2 is no. of inputs, 3 is no. of neurons in a layer\n",
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x) # different ouput from neuron as we initialize diff weights each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.40447442034189685),\n",
       " Value(data=0.3270661753589519),\n",
       " Value(data=-0.8589789507011809),\n",
       " Value(data=-0.21683436190365502),\n",
       " Value(data=-0.1373194731128462),\n",
       " Value(data=-0.13057972780855365),\n",
       " Value(data=-0.2859234903959169),\n",
       " Value(data=-0.18513984249606596),\n",
       " Value(data=0.29122764752528085),\n",
       " Value(data=0.926525469528384),\n",
       " Value(data=-0.006208995348336188),\n",
       " Value(data=0.4044028454736859),\n",
       " Value(data=-0.4298838237462337),\n",
       " Value(data=0.884838759277786),\n",
       " Value(data=-0.10500733254512351),\n",
       " Value(data=-0.7374267639432341),\n",
       " Value(data=0.47718070013562697),\n",
       " Value(data=0.3429809745901793),\n",
       " Value(data=0.7930730034321869),\n",
       " Value(data=0.6369096766213207),\n",
       " Value(data=-0.39720584514831647),\n",
       " Value(data=0.35099839520421106),\n",
       " Value(data=0.15553684131718293),\n",
       " Value(data=-0.8461602928926861),\n",
       " Value(data=0.5600956069398904),\n",
       " Value(data=-0.3873631052707234),\n",
       " Value(data=-0.054073187280274215),\n",
       " Value(data=-0.09009851903098798),\n",
       " Value(data=0.23036389612346886),\n",
       " Value(data=0.6989439964894515),\n",
       " Value(data=0.2569915242416865),\n",
       " Value(data=-0.4611428572849541),\n",
       " Value(data=0.2897937447097665),\n",
       " Value(data=0.29079268262497826),\n",
       " Value(data=0.5226356009498347),\n",
       " Value(data=-0.24387674948981153),\n",
       " Value(data=0.5775066172283032),\n",
       " Value(data=0.20337569786167609),\n",
       " Value(data=0.5275784864063842),\n",
       " Value(data=-0.10704029333851905),\n",
       " Value(data=0.06679269541445465)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.parameters() # total no. of weights and biases in the MLP\n",
    "# len(n.parameter()) # total of 41 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets draw the MLP\n",
    "draw_dot(n(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now design a binary classifier or use our micrograd\n",
    "xs = [\n",
    "    [2.0, 3.0, 1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=1.7732768129779868)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets define the loss now 'a single no. to very the nn' - mean squared error loss\n",
    "# ygt is the ground truth and yout is the prediction, we get list of losses here for each input. square is used to make the output positive\n",
    "# final loss is the sum of all losses in the list\n",
    "ypred = [n(x) for x in xs] # we are still using the MLP defined above i.e. MLP(3, [4, 4, 1]), 4 time MLP is run here i.e. 4 forward passes\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]) # 4 forward passes extended to this math expression\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.07536987698966498)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss after nudging the params (run the below cells, to see the result here) -> loss should reduce\n",
    "ypred = [n(x) for x in xs] # we are still using the MLP defined above i.e. MLP(3, [4, 4, 1]), 4 time MLP is run here i.e. 4 forward passes\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]) # 4 forward passes extended to this math expression\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets reduce the loss and we get what we want to predict\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4412833558154417"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# something magical happened above, lets check the weights on first layer's first neuron\n",
    "n.layers[0].neurons[0].w[0].grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.40447442034189685"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss is the defined in the expression above, draw_dot will draw the forward pass of the loss\n",
    "draw_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO what happened above is following\n",
    "1. 4 forward MLP passes\n",
    "2. 4 outputs produced by each run of MLP\n",
    "3. all 4 outputs combined, the loss expression is then forward passed\n",
    "4. we have final single value (L)\n",
    "\n",
    "Now time to change the grad of weights or bias, changing grad for inputs doesnt make sense since the input never changes!!! Overall to reduce L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now iterate all the params and change them\n",
    "# remember that \n",
    "for p in n.parameters():\n",
    "    # direction of the gradient (or vector of all gradients) is in the direction of the Loss\n",
    "    \n",
    "    # change in L w.r.t to a node is indicated by node's gradient. If grad is -tive, reducing the nodes data will increase the L, \n",
    "    # if the grad is +tive, changing the nodes data positively will increase L. Therefore we need to add a minus sign to the step size, because we want to minimize the L.\n",
    "    \n",
    "    # modify p.data slightly in the direction of the gradient, 0.01 is the small step size we will be taking, '-' because we want to minimize loss\n",
    "    p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.40006158678374243"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after nudging the params above, we will see a small change in data of the neuron\n",
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9999999964210621),\n",
       " Value(data=-0.9999907053746284),\n",
       " Value(data=-0.9999999503491005),\n",
       " Value(data=0.9999999960438181)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Value(data=8.639263582609676e-11)\n",
      "1 Value(data=8.639262678652186e-11)\n",
      "2 Value(data=8.639261774696947e-11)\n",
      "3 Value(data=8.63926087073963e-11)\n",
      "4 Value(data=8.639259966784485e-11)\n",
      "5 Value(data=8.639259062827185e-11)\n",
      "6 Value(data=8.639258158869929e-11)\n",
      "7 Value(data=8.639257254914928e-11)\n",
      "8 Value(data=8.639256350957767e-11)\n",
      "9 Value(data=8.63925544700286e-11)\n"
     ]
    }
   ],
   "source": [
    "# rerunning cell 20 18 19 i.e. forward pass, backward, nudging -> L reduces -> predictions becoming better\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "\n",
    "    # backward pass\n",
    "    for p in n.parameters(): # ensuring the grad is flushed before every backward pass (a.k.a zero grad)\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    # change\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "    print(k, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following is what we did\n",
    "# create the datastructure\n",
    "# create the graph method\n",
    "# create the MLP\n",
    "# create the training loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
